# -*- coding: utf-8 -*-
"""PCA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LOAhFawTPYnJQRvNQcQaYB7fRA-OtWVH
"""

url = 'https://raw.githubusercontent.com/vishwanathmankala24/Datasets/master/wine.csv'

!pip install pandas
!pip install numpy
!pip install sklearn
!pip install matplotlib

import pandas as pd
df = pd.read_csv(url)

df

df.describe()

df.head()

from sklearn.decomposition import PCA
import numpy as np

import matplotlib.pyplot as plt
from sklearn.preprocessing import scale

DF = df.values
DF

## Normalising the data

df_norm = scale(DF)

pca = PCA(n_components=13)

pca_values = pca.fit_transform(df_norm)

pca_values

var = pca.explained_variance_ratio_

var

pca.components_[0]

var1 = np.cumsum(np.round(var,decimals=4)*100)
var1

plt.plot(var1,color="red")

x = pca_values[ :, 0:1]

y = pca_values[ :, 1:2]

z = pca_values[ : 2:3]

plt.scatter(x,y,color = ["red","blue"])

from sklearn.preprocessing import StandardScaler

df_standard = StandardScaler().fit_transform(df)

df_standard

pca2 = PCA(n_components=6)

principalComponents = pca2.fit_transform(df_standard)

principalDf = pd.DataFrame(data = principalComponents, columns = ['PC1','PC2','PC3','PC4','PC5','PC6'])

var2 = pca2.explained_variance_ratio_

var2

var2 = np.cumsum(np.round(var,decimals=4)*100)
var2

df

frames = ['df','principalDf']

df

principalDf

frames = pd.concat([df, principalDf], axis=1, join='inner')
#frames = pd.concat([df, principalDf],axis=0, join='outer')
frames

p_df = principalDf[['PC1','PC2','PC3']]
p_df

### As mentioned in the problem statement, let us select only 3 columns
frames = pd.concat([df, p_df], axis=1, join='inner')
frames

## For Hierarchical Clustering

def norm_func(i):
    x = (i-i.mean())/(i.std())
    return (x)

df_norm = norm_func(frames.iloc[:,14:])

from scipy.cluster.hierarchy import linkage 
import scipy.cluster.hierarchy as sch

type(df_norm)

z = linkage(df_norm, method="complete",metric="euclidean")

plt.figure(figsize=(15, 5));plt.title('Hierarchical Clustering Dendrogram');plt.xlabel('Index');plt.ylabel('Distance')
sch.dendrogram(
    z,
    leaf_rotation=0.,  # rotates the x axis labels
    leaf_font_size=8.,  # font size for the x axis labels
)
plt.show()

# Now applying AgglomerativeClustering choosing 3 as clusters from the dendrogram
from	sklearn.cluster	import	AgglomerativeClustering 
h_complete	=	AgglomerativeClustering(n_clusters=3,	linkage='complete',affinity = "euclidean").fit(df_norm)

cluster_labels=pd.Series(h_complete.labels_)

frames['clust']=cluster_labels # creating a  new column and assigning it to new column 
frames

frames = frames.iloc[:,[17,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]
frames

# getting aggregate mean of each cluster
frames.iloc[:,2:].groupby(frames.clust).median()
## Cluster 2 is more suitable

# creating a csv file 
frames.to_csv("PCA.csv",encoding="utf-8")

### K-Means clustering for the dataset frames
###### screw plot or elbow curve ############
from	sklearn.cluster	import	KMeans
from scipy.spatial.distance import cdist 

k = list(range(2,15))
k
TWSS = [] # variable for storing total within sum of squares for each kmeans 
for i in k:
    kmeans = KMeans(n_clusters = i)
    kmeans.fit(df_norm)
    WSS = [] # variable for storing within sum of squares for each cluster 
    for j in range(i):
        WSS.append(sum(cdist(df_norm.iloc[kmeans.labels_==j,:],kmeans.cluster_centers_[j].reshape(1,df_norm.shape[1]),"euclidean")))
    TWSS.append(sum(WSS))

# Scree plot 
plt.plot(k,TWSS, 'ro-');plt.xlabel("No_of_Clusters");plt.ylabel("total_within_SS");plt.xticks(k)

model=KMeans(n_clusters=5) 
model.fit(df_norm)

model.labels_ # getting the labels of clusters assigned to each row 
md=pd.Series(model.labels_)  # converting numpy array into pandas series object 
frames['clust']=md # creating a  new column and assigning it to new column 
df_norm.head()
frames

frames = frames.iloc[:,[3,0,1,2,4,5,6,7,8,9,10,11,12,13,14,15,16,17]]
frames

frames.iloc[:,1:].groupby(frames.clust).mean()